}
predictions <- evaluateAUC(dfEvaluateOrig)
stopCluster(cl)
predictions$pred
predictions$method
predictions$modelInfo
ncol(dataTest)
dataTest[, 4640]
predictions <- predict(xgBoost_Model, dataTest[, -ncol(dataTest)], type = "prob")
predictions[[2]]
?auc
auc(ifelse(dataTest[ , "cluster"] == 1, 1, 0), predictions[[1]])
ifelse(dataTest[ , "cluster"] == 1, 1, 0)
dataTest[, "cluster"]
auc(ifelse(dataTest[, "cluster"] == "Yes", 1, 0), predictions[[1]])
evaluateAUC(dfEvaluatePCA)
evaluateAUC(dfEvaluatePCA)
evaluateAUC <- function(dfEvaluateOrig) {
set.seed(1)
trainIndex <- createDataPartition(dfEvaluateOrig$cluster, p = 0.8, list = F, times = 1)
dataTrain <- dfEvaluateOrig[trainIndex, ]
dataTest <- dfEvaluateOrig[-trainIndex, ]
controlParameters <- trainControl(method = "cv",
number = 5, # number of folds
verboseIter = TRUE, # logical for printing training log
returnData = FALSE, # saves data to slot called trainingData
returnResamp = "all", # save resampled performance measures
classProbs = TRUE, # set to TRUE for class probabilities to be computed
summaryFunction = twoClassSummary, # performance summaries
allowParallel = F) # should back end parallel processing clusters be used
parametersGrid <-  expand.grid(nrounds = 10, # number of iterations the model runs
eta = 0.3, # learning rate which is step size shrinkage which actually shrinks the feature weights
gamma = 1, # minimum loss reduction required to make a further partition on a leaf node of the tree
max_depth = 6, # how big of a tree to create
min_child_weight= 1, # minimum Sum of Instance Weight
colsample_bytree= 0.8, # randomly choosing the number of columns out of all columns during tree building process
subsample = 1) # part of data instances to grow tree
# Register parallel processing back end
cl <- makeCluster(3)
registerDoParallel(cl)
# Train model
xgBoost_Model <- train(cluster ~ ., data = dataTrain,
method = "xgbTree", metric = "ROC",
trControl = controlParameters,
tuneGrid = parametersGrid)
# Close cluster
stopCluster(cl)
# Calculate probabilities
predictions <- predict(xgBoost_Model, dataTest[, -ncol(dataTest)], type = "prob")
# Evaluate performance
print(auc(ifelse(dataTest[, "cluster"] == "Yes", 1, 0), predictions[[1]]))
}
evaluateAUC(dfEvaluatePCA)
evaluateAUC(dfEvaluateOrig)
tStart <- Sys.time()
tEnd <- Sys.time()
tEnd - tStart
as.numeric(tStart)
tStart
evaluateAUC <- function(dfEvaluateOrig) {
set.seed(1)
tStart <- Sys.time()
trainIndex <- createDataPartition(dfEvaluateOrig$cluster, p = 0.8, list = F, times = 1)
dataTrain <- dfEvaluateOrig[trainIndex, ]
dataTest <- dfEvaluateOrig[-trainIndex, ]
controlParameters <- trainControl(method = "cv",
number = 5, # number of folds
verboseIter = TRUE, # logical for printing training log
returnData = FALSE, # saves data to slot called trainingData
returnResamp = "all", # save resampled performance measures
classProbs = TRUE, # set to TRUE for class probabilities to be computed
summaryFunction = twoClassSummary, # performance summaries
allowParallel = F) # should back end parallel processing clusters be used
parametersGrid <-  expand.grid(nrounds = 10, # number of iterations the model runs
eta = 0.3, # learning rate which is step size shrinkage which actually shrinks the feature weights
gamma = 1, # minimum loss reduction required to make a further partition on a leaf node of the tree
max_depth = 6, # how big of a tree to create
min_child_weight= 1, # minimum Sum of Instance Weight
colsample_bytree= 0.8, # randomly choosing the number of columns out of all columns during tree building process
subsample = 1) # part of data instances to grow tree
# Register parallel processing back end
cl <- makeCluster(3)
registerDoParallel(cl)
# Train model
xgBoost_Model <- train(cluster ~ ., data = dataTrain,
method = "xgbTree", metric = "ROC",
trControl = controlParameters,
tuneGrid = parametersGrid)
# Close cluster
stopCluster(cl)
# Calculate probabilities
predictions <- predict(xgBoost_Model, dataTest[, -ncol(dataTest)], type = "prob")
# Evaluate performance
tEnd <- Sys.time()
print(auc(ifelse(dataTest[, "cluster"] == "Yes", 1, 0), predictions[[1]]))
print(tEnd - tStart)
}
evaluateAUC(dfEvaluateOrig)
evaluateAUC(dfEvaluatePCA)
timeAlgorithm <- function(algorithm) {
tStart <- Sys.time()
algorithm
tEnd <- Sys.time()
tEnd - tStart
}
evaluateAUC(dfEvaluateOrig) %>% timeAlgorithm
library(tidyr)
evaluateAUC(dfEvaluateOrig) %>% timeAlgorithm
evaluateAUC <- function(dfEvaluateOrig) {
set.seed(1)
trainIndex <- createDataPartition(dfEvaluateOrig$cluster, p = 0.8, list = F, times = 1)
dataTrain <- dfEvaluateOrig[trainIndex, ]
dataTest <- dfEvaluateOrig[-trainIndex, ]
controlParameters <- trainControl(method = "cv",
number = 5, # number of folds
verboseIter = TRUE, # logical for printing training log
returnData = FALSE, # saves data to slot called trainingData
returnResamp = "all", # save resampled performance measures
classProbs = TRUE, # set to TRUE for class probabilities to be computed
summaryFunction = twoClassSummary, # performance summaries
allowParallel = F) # should back end parallel processing clusters be used
parametersGrid <-  expand.grid(nrounds = 10, # number of iterations the model runs
eta = 0.3, # learning rate which is step size shrinkage which actually shrinks the feature weights
gamma = 1, # minimum loss reduction required to make a further partition on a leaf node of the tree
max_depth = 6, # how big of a tree to create
min_child_weight= 1, # minimum Sum of Instance Weight
colsample_bytree= 0.8, # randomly choosing the number of columns out of all columns during tree building process
subsample = 1) # part of data instances to grow tree
# Register parallel processing back end
cl <- makeCluster(3)
registerDoParallel(cl)
# Train model
xgBoost_Model <- train(cluster ~ ., data = dataTrain,
method = "xgbTree", metric = "ROC",
trControl = controlParameters,
tuneGrid = parametersGrid)
# Close cluster
stopCluster(cl)
# Calculate probabilities
predictions <- predict(xgBoost_Model, dataTest[, -ncol(dataTest)], type = "prob")
# Evaluate performance
print(auc(ifelse(dataTest[, "cluster"] == "Yes", 1, 0), predictions[[1]]))
}
timeAlgorithm <- function(algorithm) {
tStart <- Sys.time()
algorithm
tEnd <- Sys.time()
tEnd - tStart
}
evaluateAUC(dfEvaluatePCA) %>% timeAlgorithm
evaluateAUC <- function(dfEvaluateOrig) {
set.seed(1)
trainIndex <- createDataPartition(dfEvaluateOrig$cluster, p = 0.8, list = F, times = 1)
dataTrain <- dfEvaluateOrig[trainIndex, ]
dataTest <- dfEvaluateOrig[-trainIndex, ]
controlParameters <- trainControl(method = "cv",
number = 5, # number of folds
verboseIter = FALSE, # logical for printing training log
returnData = FALSE, # saves data to slot called trainingData
returnResamp = "all", # save resampled performance measures
classProbs = TRUE, # set to TRUE for class probabilities to be computed
summaryFunction = twoClassSummary, # performance summaries
allowParallel = F) # should back end parallel processing clusters be used
parametersGrid <-  expand.grid(nrounds = 10, # number of iterations the model runs
eta = 0.3, # learning rate which is step size shrinkage which actually shrinks the feature weights
gamma = 1, # minimum loss reduction required to make a further partition on a leaf node of the tree
max_depth = 6, # how big of a tree to create
min_child_weight= 1, # minimum Sum of Instance Weight
colsample_bytree= 0.8, # randomly choosing the number of columns out of all columns during tree building process
subsample = 1) # part of data instances to grow tree
# Register parallel processing back end
cl <- makeCluster(3)
registerDoParallel(cl)
# Train model
xgBoost_Model <- train(cluster ~ ., data = dataTrain,
method = "xgbTree", metric = "ROC",
trControl = controlParameters,
tuneGrid = parametersGrid)
# Close cluster
stopCluster(cl)
# Calculate probabilities
predictions <- predict(xgBoost_Model, dataTest[, -ncol(dataTest)], type = "prob")
# Evaluate performance
print(auc(ifelse(dataTest[, "cluster"] == "Yes", 1, 0), predictions[[1]]))
}
timeAlgorithm <- function(algorithm) {
tStart <- Sys.time()
algorithm
tEnd <- Sys.time()
tEnd - tStart
}
evaluateAUC(dfEvaluatePCA) %>% timeAlgorithm
princ
dfComponents
princ[, 1:20]
princ$x
predict(princ, newdata = scale(data))
summary(princ)
princ
plot(princ)
pve <- princ$sdev^2/sum(princ$sdev^2)
pve
head(pve)
cumsum(pve)
pve <- princ$sdev^2 / sum(princ$sdev^2)
cumsum(pve)
plot(cumsum(pve))
dfComponents <- predict(princ, newdata = scale(data))[, 1:nComp]
nComp <- 1000
dfComponents <- predict(princ, newdata = scale(data))[, 1:nComp]
dfComponents <- predict(princ, newdata = scale(gisette_nzv))[, 1:nComp]
dfEvaluatePCA <- cbind(as.data.frame(dfComponents), cluster = g_labels)
evaluateAUC(dfEvaluatePCA) %>% timeAlgorithm
timeAlgorithm <- function(algorithm) {
tStart <- Sys.time()
algorithm
tEnd <- Sys.time()
difftime(tStart, tEnd)
}
evaluateAUC(dfEvaluatePCA) %>% timeAlgorithm
timeAlgorithm <- function(algorithm) {
tStart <- Sys.time()
algorithm
tEnd <- Sys.time()
difftime(tEnd, tStart, units = "secs")
}
evaluateAUC(dfEvaluatePCA) %>% timeAlgorithm
timeAlgorithm(evaluateAUC(dfEvaluatePCA))
timeAlgorithm(evaluateAUC(dfEvaluatePCA))
timeAlgorithm(evaluateAUC(dfEvaluatePCA))
evaluateAUC <- function(dfEvaluateOrig) {
set.seed(1)
trainIndex <- createDataPartition(dfEvaluateOrig$cluster, p = 0.8, list = F, times = 1)
dataTrain <- dfEvaluateOrig[trainIndex, ]
dataTest <- dfEvaluateOrig[-trainIndex, ]
controlParameters <- trainControl(method = "cv",
number = 5, # number of folds
verboseIter = TRUE, # logical for printing training log
returnData = FALSE, # saves data to slot called trainingData
returnResamp = "all", # save resampled performance measures
classProbs = TRUE, # set to TRUE for class probabilities to be computed
summaryFunction = twoClassSummary, # performance summaries
allowParallel = F) # should back end parallel processing clusters be used
parametersGrid <-  expand.grid(nrounds = 10, # number of iterations the model runs
eta = 0.3, # learning rate which is step size shrinkage which actually shrinks the feature weights
gamma = 1, # minimum loss reduction required to make a further partition on a leaf node of the tree
max_depth = 6, # how big of a tree to create
min_child_weight= 1, # minimum Sum of Instance Weight
colsample_bytree= 0.8, # randomly choosing the number of columns out of all columns during tree building process
subsample = 1) # part of data instances to grow tree
# Register parallel processing back end
cl <- makeCluster(3)
registerDoParallel(cl)
# Train model
xgBoost_Model <- train(cluster ~ ., data = dataTrain,
method = "xgbTree", metric = "ROC",
trControl = controlParameters,
tuneGrid = parametersGrid)
# Close cluster
stopCluster(cl)
# Calculate probabilities
predictions <- predict(xgBoost_Model, dataTest[, -ncol(dataTest)], type = "prob")
# Evaluate performance
print(auc(ifelse(dataTest[, "cluster"] == "Yes", 1, 0), predictions[[1]]))
}
timeAlgorithm(evaluateAUC(dfEvaluateOrig))
pve[1000]
cumsum(pve[1:1000])
cumsum(pve[1000])
cumsum(pve[1:1000])
?gafsControl
evaluateAUC <- function(dfEvaluateOrig) {
set.seed(1)
trainIndex <- createDataPartition(dfEvaluateOrig$cluster, p = 0.8, list = F, times = 1)
dataTrain <- dfEvaluateOrig[trainIndex, ]
dataTest <- dfEvaluateOrig[-trainIndex, ]
controlParameters <- trainControl(method = "cv",
number = 5, # number of folds
verboseIter = TRUE, # logical for printing training log
returnData = FALSE, # saves data to slot called trainingData
returnResamp = "all", # save resampled performance measures
classProbs = TRUE, # set to TRUE for class probabilities to be computed
summaryFunction = twoClassSummary, # performance summaries
allowParallel = T) # should back end parallel processing clusters be used
parametersGrid <-  expand.grid(nrounds = 10, # number of iterations the model runs
eta = 0.3, # learning rate which is step size shrinkage which actually shrinks the feature weights
gamma = 1, # minimum loss reduction required to make a further partition on a leaf node of the tree
max_depth = 6, # how big of a tree to create
min_child_weight= 1, # minimum Sum of Instance Weight
colsample_bytree= 0.8, # randomly choosing the number of columns out of all columns during tree building process
subsample = 1) # part of data instances to grow tree
# Register parallel processing back end
cl <- makeCluster(3)
registerDoParallel(cl)
# Train model
xgBoost_Model <- train(cluster ~ ., data = dataTrain,
method = "xgbTree", metric = "ROC",
trControl = controlParameters,
tuneGrid = parametersGrid)
# Close cluster
stopCluster(cl)
# Calculate probabilities
predictions <- predict(xgBoost_Model, dataTest[, -ncol(dataTest)], type = "prob")
# Evaluate performance
print(auc(ifelse(dataTest[, "cluster"] == "Yes", 1, 0), predictions[[1]]))
}
timeAlgorithm(evaluateAUC(dfEvaluateOrig))
timeAlgorithm(evaluateAUC(dfEvaluatePCA))
ga_ctrl <- gafsControl(method = "cv",
number = 5,
allowParallel = TRUE)
rf_ga <- gafs(cluster ~ ., data = dataTrain,
iters = 100, gafsControl = ga_ctrl,
method = "xgbTree", metric = "ROC")
?gafs
rf_ga <- gafs(x = dataTrain[, -ncol(dataTrain)], y = dataTrain$cluster,
iters = 100, gafsControl = ga_ctrl,
method = "xgbTree", metric = "ROC")
ga_ctrl <- gafsControl(functions = rfGA,
method = "cv",
metric = "ROC"
number = 5,
allowParallel = TRUE)
ga_ctrl <- gafsControl(functions = rfGA,
method = "cv",
metric = "ROC",
number = 5,
allowParallel = TRUE)
ga_ctrl <- gafsControl(functions = rfGA,
method = "cv",
number = 5,
metric = c(internal = "D", external = "RMSE"),
maximize = c(internal = TRUE, external = FALSE)),
allowParallel = TRUE)
ga_ctrl <- gafsControl(functions = rfGA,
method = "cv",
number = 5,
metric = c(internal = "D", external = "RMSE"),
maximize = c(internal = TRUE, external = FALSE)),
allowParallel = TRUE)
o
ga_ctrl <- gafsControl(functions = rfGA,
method = "cv",
number = 5,
metric = c(internal = "D", external = "RMSE"),
maximize = c(internal = TRUE, external = FALSE)),
allowParallel = TRUE)
ga_ctrl <- gafsControl(functions = rfGA,
method = "cv",
number = 5,
metric = c(internal = "D", external = "RMSE"),
maximize = c(internal = TRUE, external = FALSE),
allowParallel = TRUE)
rf_ga <- gafs(x = dataTrain[, -ncol(dataTrain)], y = dataTrain$cluster,
iters = 100, gafsControl = ga_ctrl)
ga_ctrl <- gafsControl(functions = rfGA,
method = "cv",
number = 5,
metric = c(internal = "D", external = "Accuracy"),
maximize = c(internal = TRUE, external = FALSE),
allowParallel = TRUE)
rf_ga <- gafs(x = dataTrain[, -ncol(dataTrain)], y = dataTrain$cluster,
iters = 100, gafsControl = ga_ctrl)
cl <- makeCluster(3)
registerDoParallel(cl)
rf_ga <- gafs(x = dataTrain[, -ncol(dataTrain)], y = dataTrain$cluster,
iters = 100, gafsControl = ga_ctrl)
stopCluster(cl)
stopCluster()
library(doParallel)
stopCluster()
career <- Batting %>%
filter(AB > 0) %>%
anti_join(Pitching, by = "playerID") %>%
group_by(player_ID) %>%
summarize(H = sum(H), AB = sum(AB)) %>%
mutate(average = H/AB)
library(dplyr)
library(tidyr)
library(ggplot2)
library(Lahman)
# Empirical Bayes Estimation
career <- Batting %>%
filter(AB > 0) %>%
anti_join(Pitching, by = "playerID") %>%
group_by(player_ID) %>%
summarize(H = sum(H), AB = sum(AB)) %>%
mutate(average = H/AB)
career <- Batting %>%
filter(AB > 0) %>%
anti_join(Pitching, by = "playerID") %>%
group_by(playerID) %>%
summarize(H = sum(H), AB = sum(AB)) %>%
mutate(average = H/AB)
career <- Master %>%
tbl_df() %>%
dplyr::select(playerID, nameFirst, nameLast) %>%
unite(name, nameFirst, nameLast, sep = " ") %>%
inner_join(career, by = "playerID") %>%
dplyr::select(-playerID)
library(stats4)
career_filter <- career %>% filter(AB > 500)
log_likelihood <- function(x) {
x <- career_filter$H
total <- career_filter$AB
-sum(VGAM::dbetabinom.ab(x, total, alpha, beta, log = TRUE))
}
maximumLikelihood <- mle(log_likelihood, start = list(alpha = 1, beta = 10),
method = "L-BFGS-B", lower = c(0.0001, 0.1))
log_likelihood <- function(alpha, beta) {
x <- career_filter$H
total <- career_filter$AB
-sum(VGAM::dbetabinom.ab(x, total, alpha, beta, log = TRUE))
}
maximumLikelihood <- mle(log_likelihood, start = list(alpha = 1, beta = 10),
method = "L-BFGS-B", lower = c(0.0001, 0.1))
library(stats4)
log_likelihood <- function(alpha, beta) {
x <- career_filter$H
total <- career_filter$AB
-sum(VGAM::dbetabinom.ab(x, total, alpha, beta, log = TRUE))
}
maximumLikelihood <- mle(log_likelihood, start = list(alpha = 1, beta = 10),
method = "L-BFGS-B", lower = c(0.0001, 0.1))
library(VGAM)
install.packages("VGAM")
# Step 1: Estimate a prior from all data
library(stats4)
career_filter <- career %>% filter(AB > 500)
log_likelihood <- function(alpha, beta) {
x <- career_filter$H
total <- career_filter$AB
-sum(VGAM::dbetabinom.ab(x, total, alpha, beta, log = TRUE))
}
maximumLikelihood <- mle(log_likelihood, start = list(alpha = 1, beta = 10),
method = "L-BFGS-B", lower = c(0.0001, 0.1))
ab <- coef(maximumLikelihood)
alpha0 <- ab[1]
beta0 <- ab[2]
career_eb <- career %>%
mutate(eb_estimate = (H + alpha0) / (AB + alpha0 + beta0))
career_eb
ggplot(career_eb, aes(x = average, y = eb_estimate)) + geom_point()
?geom_hline
ggplot(career_eb, aes(x = average, y = eb_estimate)) + geom_point() +
geom_hline(yintercept = (alpha0 / alpha0 + beta0), col = "red") +
geom_abline(intercept = 0, slope = 1)
ggplot(career_eb, aes(x = average, y = eb_estimate)) + geom_point()
alpha0 / alpha0+beta0
alpha0 / (alpha0+beta0)
ggplot(career_eb, aes(x = average, y = eb_estimate)) + geom_point() +
geom_hline(yintercept = alpha0 / (alpha0 + beta0), col = "red")
ggplot(career_eb, aes(x = average, y = eb_estimate)) + geom_point() +
geom_hline(yintercept = alpha0 / (alpha0 + beta0), col = "red", linetype = "dashed") +
geom_abline(intercept = 0, slope = 1)
ggplot(career_eb, aes(x = average, y = eb_estimate)) + geom_point() +
geom_hline(yintercept = alpha0 / (alpha0 + beta0), col = "red", linetype = "dashed") +
geom_abline(intercept = 0, slope = 1, col = "red")
career_eb <- career_eb %>%
mutate(alpha1 <- alpha0 + H,
beta1 <- beta0 + AB - H)
yankee_1998_career <- yankee_1998_career %>%
mutate(low = qbeta(.025, alpha1, beta1),
high = qbeta(.975, alpha1, beta1))
yankee_1998_career <- career_eb %>%
mutate(low = qbeta(.025, alpha1, beta1),
high = qbeta(.975, alpha1, beta1))
career_eb <- career_eb %>%
mutate(alpha1 <- alpha0 + H,
beta1 <- beta0 + AB - H)
yankee_1998_career <- career_eb %>%
mutate(low = qbeta(.025, alpha1, beta1),
high = qbeta(.975, alpha1, beta1))
career_eb <- career_eb %>%
mutate(alpha1 = alpha0 + H,
beta1 = beta0 + AB - H)
yankee_1998_career <- career_eb %>%
mutate(low = qbeta(.025, alpha1, beta1),
high = qbeta(.975, alpha1, beta1))
yankee_1998_career %>%
mutate(name = reorder(name, eb_estimate)) %>%
ggplot(aes(eb_estimate, name)) +
geom_point() +
geom_errorbarh(aes(xmin = low, xmax = high)) +
geom_vline(xintercept = alpha0 / (alpha0 + beta0), color = "red", lty = 2) + xlab("Estimated batting average (w/ 95% interval)") +
ylab("Player")
str(career_eb)
unique(career_eb$name)
yankee_1998_career <- career_eb %>%
filter(name %in% c("Bernie Williams", "Chuck Knoblauch", "Darryl Strawberry",
"Derek Jeter", "Jorge Posada", "Scott Brosius", "Tino Martinez"))
yankee_1998_career <- career_eb %>%
mutate(low = qbeta(.025, alpha1, beta1),
high = qbeta(.975, alpha1, beta1))
yankee_1998_career %>%
mutate(name = reorder(name, eb_estimate)) %>%
ggplot(aes(eb_estimate, name)) +
geom_point() +
geom_errorbarh(aes(xmin = low, xmax = high)) +
geom_vline(xintercept = alpha0 / (alpha0 + beta0), color = "red", lty = 2) + xlab("Estimated batting average (w/ 95% interval)") +
ylab("Player")
getwd()
install.packages("wordcloud")
library(wordcloud)
.libPaths(
)
getwd()
setwd("/Users/MichaelChoie/Desktop/Deep_Learning/11. generate_tv_script/")
setwd("/Users/MichaelChoie/Desktop/Deep_Learning/11. generate_tv_script/data")
l <- readLines("moes_tavern_lines.txt")
l
library(wordcloud)
